{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For Data Preparation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # regular expressions\n",
    "\n",
    "\n",
    "# To clean up texts\n",
    "import nltk.data\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "# For Word Embedding\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import gensim.models as g\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# For the Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Bidirectional,Dropout, Input, SpatialDropout1D, CuDNNLSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "\n",
    "import logging\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "IMDB_train = pd.read_csv('./IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)\n",
    "IMDB_train=preprocessing(IMDB_train)\n",
    "IMDB_test=preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sentence into a list of words\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # Convert words to lower case and split them\n",
    "    words = sentence.lower().split()\n",
    "    # Lemmatizing\n",
    "    #words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # 6. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole data into a list of sentences where each sentence is a list of word items\n",
    "def list_of_sentences(data):\n",
    "    sentences = []\n",
    "    for i in data:\n",
    "        sentences.append(sentence_to_wordlist(i))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list_of_sentences(IMDB_train)\n",
    "train_y = IMDB_train_y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 35674\n"
     ]
    }
   ],
   "source": [
    "# Create Word Vectors\n",
    "\n",
    "wv_model = Word2Vec(size=128, window=5, min_count=4, workers=4)\n",
    "\n",
    "wv_model.build_vocab(train_x) \n",
    "wv_model.train(train_x, total_examples=wv_model.corpus_count, epochs=wv_model.iter)\n",
    "word_vectors = wv_model.wv\n",
    "words = list(wv_model.wv.vocab)\n",
    "\n",
    "# Calling init_sims will make the model will be better for memory\n",
    "# if we don't want to train the model over and over again\n",
    "wv_model.init_sims(replace=True)\n",
    "\n",
    "#n_words = print(len(words))\n",
    "\n",
    "print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))\n",
    "\n",
    "# save model\n",
    "wv_model.wv.save_word2vec_format('model.txt', binary=False)\n",
    "\n",
    "# load model\n",
    "#new_model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "new_model = KeyedVectors.load_word2vec_format('model.txt')\n",
    "#model.save_word2vec_format('model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = KeyedVectors.load_word2vec_format('model.txt')\n",
    "retrofitted_model = KeyedVectors.load_word2vec_format('out_vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_words = list(retrofitted_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary & inv_vocab\n",
    "\n",
    "def create_vocab(data_collect, max_vocab):\n",
    "    # Get raw data\n",
    "    x_list = data_collect\n",
    "    sample_count = sum([len(x) for x in x_list])\n",
    "    words = []\n",
    "    for data in x_list:\n",
    "        words.extend([data])\n",
    "    count = Counter(words) # word count\n",
    "    inv_vocab = [x[0] for x in count.most_common(max_vocab)]\n",
    "    vocab = {x: i for i, x in enumerate(inv_vocab, 1)}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = create_vocab(words, len(words))\n",
    "ret_vocab, ret_inv_vocab = create_vocab(new_words, len(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max length sentence\n",
    "def find_max_length_sentence(sentence):\n",
    "    max_length = 0\n",
    "    for i in sentence:\n",
    "        length = len(sentence_to_wordlist(i))\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = find_max_length_sentence(IMDB_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each word to corresponding vector\n",
    "def map_to_vec(word):\n",
    "    vec = wv_model[word]\n",
    "    return vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "def make_emb_matrix(inv_vocab):\n",
    "    emb_matrix = []\n",
    "    for word in inv_vocab:\n",
    "        emb_matrix.append(map_to_vec(word))\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embedding = np.asarray(make_emb_matrix(inv_vocab))\n",
    "ret_embedding = np.asarray(make_emb_matrix(ret_inv_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 100\n",
    "num_words = len(word_vectors.vocab)\n",
    "vocab = Counter(words)\n",
    "ret_vocab = Counter(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(num_words-1))}\n",
    "\n",
    "train_sequences = [[word_index.get(t, 0) for t in sentence]\n",
    "             for sentence in IMDB_train[:len(IMDB_train)]]\n",
    "\n",
    "test_sequences = [[word_index.get(t, 0)\n",
    "                   for t in sentence] for sentence in IMDB_test[:len(IMDB_test)]]\n",
    "\n",
    "# Pad zeros to match the size of matrix\n",
    "train_data = pad_sequences(train_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")\n",
    "test_data = pad_sequences(test_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(num_words, wv_dim) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_word_index = {t[0]: i+1 for i,t in enumerate(ret_vocab.most_common(num_words-1))}\n",
    "\n",
    "ret_train_sequences = [[ret_word_index.get(t, 0) for t in sentence]\n",
    "             for sentence in IMDB_train[:len(IMDB_train)]]\n",
    "\n",
    "ret_test_sequences = [[ret_word_index.get(t, 0)\n",
    "                   for t in sentence] for sentence in IMDB_test[:len(IMDB_test)]]\n",
    "\n",
    "# Pad zeros to match the size of matrix\n",
    "ret_train_data = pad_sequences(ret_train_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")\n",
    "ret_test_data = pad_sequences(ret_test_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the matrix with random numbers\n",
    "ret_wv_matrix = (np.random.rand(num_words, wv_dim) - 0.5) / 5.0\n",
    "for word, i in ret_word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        ret_embedding_vector = ret_word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        ret_wv_matrix[i] = ret_embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Conv2D, MaxPooling2D, Flatten, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_1(comment_input):\n",
    "    wv_layer = Embedding(num_words,\n",
    "                     wv_dim,\n",
    "                     mask_zero=False,\n",
    "                     weights=[wv_matrix],\n",
    "                     input_length=seq_length,\n",
    "                     trainable=False)\n",
    "    \n",
    "    embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "    input_layer=Reshape([2450,100,1])(embedded_sequences)\n",
    "\n",
    "    conv1=Conv2D(64,kernel_size=(5,100),activation='relu')(input_layer)\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=(2,1))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(32,kernel_size=(5,1),activation='relu')(pool1)\n",
    "\n",
    "    pool2 = MaxPooling2D(pool_size=(2,1))(conv2)\n",
    "\n",
    "    pool2_flat=Flatten()(pool2)\n",
    "\n",
    "    dropout = Dropout(0.4)(pool2_flat)\n",
    "\n",
    "    # normalize = BatchNormalization()(dropout)\n",
    "\n",
    "    logits = Dense(1, activation='sigmoid')(dropout)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def train_and_eval(cnn):\n",
    "    comment_input = Input(shape=(seq_length,), dtype='int64')\n",
    "\n",
    "    preds= cnn(comment_input)\n",
    "    model = Model(inputs=[comment_input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',   #binary_crossentropy\n",
    "                  optimizer=Adam(lr=0.0005, clipnorm=.25, beta_1=0.9, beta_2=0.999),\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    hist = model.fit(train_data, IMDB_train_y, validation_data=(test_data, IMDB_test_y), epochs=15, batch_size=32)\n",
    "    \n",
    "    y_pred_train = model.predict(train_data)\n",
    "    y_pred_train = [int(item>0.5) for  item in y_pred_train]\n",
    "    y_pred_test = model.predict(test_data)\n",
    "    y_pred_test = [int(item>0.5) for  item in y_pred_test]\n",
    "    f1_train= f1_score(IMDB_train_y, y_pred_train, average='micro')\n",
    "    f1_test= f1_score(IMDB_test_y, y_pred_test, average='micro')\n",
    "    print('f1 (train): ', f1_train)\n",
    "    print('f1 (test): ', f1_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ret(comment_input):\n",
    "    wv_layer = Embedding(num_words,\n",
    "                     wv_dim,\n",
    "                     mask_zero=False,\n",
    "                     weights=[ret_wv_matrix],\n",
    "                     input_length=seq_length,\n",
    "                     trainable=False)\n",
    "    \n",
    "    embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "    input_layer=Reshape([2450,100,1])(embedded_sequences)\n",
    "\n",
    "    conv1=Conv2D(64,kernel_size=(5,100),activation='relu')(input_layer)\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=(2,1))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(32,kernel_size=(5,1),activation='relu')(pool1)\n",
    "\n",
    "    pool2 = MaxPooling2D(pool_size=(2,1))(conv2)\n",
    "\n",
    "    pool2_flat=Flatten()(pool2)\n",
    "\n",
    "    dropout = Dropout(0.4)(pool2_flat)\n",
    "\n",
    "    # normalize = BatchNormalization()(dropout)\n",
    "\n",
    "    logits = Dense(1, activation='sigmoid')(dropout)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_2(comment_input):\n",
    "    wv_layer = Embedding(num_words,\n",
    "                     wv_dim,\n",
    "                     mask_zero=False,\n",
    "                     weights=[wv_matrix],\n",
    "                     input_length=seq_length,\n",
    "                     trainable=False)\n",
    "\n",
    "    ret_wv_layer = Embedding(num_words,\n",
    "                     wv_dim,\n",
    "                     mask_zero=False,\n",
    "                     weights=[ret_wv_matrix],\n",
    "                     input_length=seq_length,\n",
    "                     trainable=False)\n",
    "    # channel1\n",
    "    embedded_sequences1 = wv_layer(comment_input)\n",
    "\n",
    "    input_layer_1=Reshape([2450,100,1])(embedded_sequences1)\n",
    "\n",
    "    conv1_1=Conv2D(64,kernel_size=(5,100),activation='relu')(input_layer_1)\n",
    "\n",
    "    pool1_1 = MaxPooling2D(pool_size=(2,1))(conv1_1)\n",
    "\n",
    "    conv2_1 = Conv2D(32,kernel_size=(5,1),activation='relu')(pool1_1)\n",
    "\n",
    "    pool2_1 = MaxPooling2D(pool_size=(2,1))(conv2_1)\n",
    "\n",
    "    pool2_flat_1=Flatten()(pool2_1)\n",
    "\n",
    "    # channel 2\n",
    "    embedded_sequences2 = ret_wv_layer(comment_input)\n",
    "    input_layer_2=Reshape([2450,100,1])(embedded_sequences2)\n",
    "\n",
    "    conv1_2=Conv2D(64,kernel_size=(5,100),activation='relu')(input_layer_2)\n",
    "\n",
    "    pool1_2 = MaxPooling2D(pool_size=(2,1))(conv1_2)\n",
    "\n",
    "    conv2_2 = Conv2D(32,kernel_size=(5,1),activation='relu')(pool1_2)\n",
    "\n",
    "    pool2_2 = MaxPooling2D(pool_size=(2,1))(conv2_2)\n",
    "\n",
    "    pool2_flat_2=Flatten()(pool2_2)\n",
    "    \n",
    "    # merge\n",
    "    merged = Concatenate(axis=-1)([pool2_flat_1, pool2_flat_2])\n",
    "    # interpretation\n",
    "    dropout_2 = Dropout(0.4)(merged)\n",
    "    \n",
    "    logits = Dense(1, activation='sigmoid')(dropout_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 2450, 100)         3567400   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 2450, 100, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2446, 1, 64)       32064     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1223, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1219, 1, 32)       10272     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 609, 1, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 19488)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 19488)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 19489     \n",
      "=================================================================\n",
      "Total params: 3,629,225\n",
      "Trainable params: 61,825\n",
      "Non-trainable params: 3,567,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 616s 25ms/step - loss: 0.6795 - acc: 0.5554 - val_loss: 0.6236 - val_acc: 0.6568\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 615s 25ms/step - loss: 0.5933 - acc: 0.6914 - val_loss: 0.5269 - val_acc: 0.7420\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 612s 24ms/step - loss: 0.5266 - acc: 0.7454 - val_loss: 0.4770 - val_acc: 0.7796\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 610s 24ms/step - loss: 0.5064 - acc: 0.7639 - val_loss: 0.4724 - val_acc: 0.7781\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 613s 25ms/step - loss: 0.4714 - acc: 0.7858 - val_loss: 0.4674 - val_acc: 0.7827\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 613s 25ms/step - loss: 0.4402 - acc: 0.8024 - val_loss: 0.4900 - val_acc: 0.7729\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 614s 25ms/step - loss: 0.4410 - acc: 0.8038 - val_loss: 0.6456 - val_acc: 0.7198\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 613s 25ms/step - loss: 0.4181 - acc: 0.8209 - val_loss: 0.7650 - val_acc: 0.6766\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 618s 25ms/step - loss: 0.3968 - acc: 0.8269 - val_loss: 0.4151 - val_acc: 0.8166\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 613s 25ms/step - loss: 0.3704 - acc: 0.8411 - val_loss: 0.4066 - val_acc: 0.8221\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 614s 25ms/step - loss: 0.3471 - acc: 0.8530 - val_loss: 0.4853 - val_acc: 0.7918\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 612s 24ms/step - loss: 0.3310 - acc: 0.8607 - val_loss: 0.4226 - val_acc: 0.8186\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 611s 24ms/step - loss: 0.3328 - acc: 0.8605 - val_loss: 0.4470 - val_acc: 0.8048\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 611s 24ms/step - loss: 0.3157 - acc: 0.8680 - val_loss: 0.4623 - val_acc: 0.8076\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 610s 24ms/step - loss: 0.3046 - acc: 0.8719 - val_loss: 0.4447 - val_acc: 0.8145\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(cnn_1) # kernel size=5, dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 2450, 100)         3567400   \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 2450, 100, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 2448, 1, 64)       19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 1224, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 1222, 1, 32)       6176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 611, 1, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 19552)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 19552)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 19553     \n",
      "=================================================================\n",
      "Total params: 3,612,393\n",
      "Trainable params: 44,993\n",
      "Non-trainable params: 3,567,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 469s 19ms/step - loss: 0.6879 - acc: 0.5317 - val_loss: 0.6593 - val_acc: 0.6014\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 461s 18ms/step - loss: 0.6188 - acc: 0.6621 - val_loss: 0.5990 - val_acc: 0.6810\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 461s 18ms/step - loss: 0.5545 - acc: 0.7241 - val_loss: 0.5263 - val_acc: 0.7436\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 461s 18ms/step - loss: 0.5238 - acc: 0.7494 - val_loss: 0.4941 - val_acc: 0.7632\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 459s 18ms/step - loss: 0.4966 - acc: 0.7645 - val_loss: 0.4833 - val_acc: 0.7742\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 460s 18ms/step - loss: 0.4860 - acc: 0.7737 - val_loss: 0.4796 - val_acc: 0.7762\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 460s 18ms/step - loss: 0.4558 - acc: 0.7918 - val_loss: 0.5137 - val_acc: 0.7579\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 460s 18ms/step - loss: 0.4283 - acc: 0.8069 - val_loss: 0.4909 - val_acc: 0.7757\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 459s 18ms/step - loss: 0.4319 - acc: 0.8061 - val_loss: 0.6870 - val_acc: 0.6883\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 459s 18ms/step - loss: 0.4125 - acc: 0.8145 - val_loss: 0.5034 - val_acc: 0.7674\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 463s 19ms/step - loss: 0.3842 - acc: 0.8305 - val_loss: 0.6012 - val_acc: 0.7435\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 461s 18ms/step - loss: 0.3851 - acc: 0.8334 - val_loss: 0.6135 - val_acc: 0.7301\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 460s 18ms/step - loss: 0.3714 - acc: 0.8382 - val_loss: 0.6462 - val_acc: 0.7340\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 459s 18ms/step - loss: 0.3422 - acc: 0.8512 - val_loss: 0.5431 - val_acc: 0.7645\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 459s 18ms/step - loss: 0.3412 - acc: 0.8533 - val_loss: 0.4836 - val_acc: 0.7839\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d512238a6d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-f7b799595837>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(cnn)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mf1_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mf1_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f1 (train): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "train_and_eval(cnn_1) # kernel size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 2450, 100)         3567400   \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 2450, 100, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 2446, 1, 64)       32064     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 1223, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 1219, 1, 32)       10272     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 609, 1, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 19488)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 19488)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 19489     \n",
      "=================================================================\n",
      "Total params: 3,629,225\n",
      "Trainable params: 61,825\n",
      "Non-trainable params: 3,567,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "  640/25000 [..............................] - ETA: 8:39 - loss: 0.7072 - acc: 0.5078"
     ]
    }
   ],
   "source": [
    "model_1=train_and_eval(cnn_1) # dropout: 0.4, kernel_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ret=train_and_eval(cnn_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2450)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2450, 100)    3567400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 2450, 100)    3567400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2450, 100, 1) 0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 2450, 100, 1) 0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2446, 1, 64)  32064       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 2446, 1, 64)  32064       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1223, 1, 64)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1223, 1, 64)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1219, 1, 32)  10272       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 1219, 1, 32)  10272       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 609, 1, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 609, 1, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 19488)        0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 19488)        0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 38976)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 38976)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            38977       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,258,449\n",
      "Trainable params: 123,649\n",
      "Non-trainable params: 7,134,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 1197s 48ms/step - loss: 0.6806 - acc: 0.5501 - val_loss: 0.6340 - val_acc: 0.6378\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 1138s 46ms/step - loss: 0.5803 - acc: 0.7001 - val_loss: 0.5279 - val_acc: 0.7420\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 1176s 47ms/step - loss: 0.5262 - acc: 0.7454 - val_loss: 0.4715 - val_acc: 0.7803\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 1167s 47ms/step - loss: 0.4872 - acc: 0.7737 - val_loss: 0.4434 - val_acc: 0.7988\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 1187s 47ms/step - loss: 0.4357 - acc: 0.8034 - val_loss: 0.5270 - val_acc: 0.7566\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 1194s 48ms/step - loss: 0.4058 - acc: 0.8174 - val_loss: 0.4168 - val_acc: 0.8136\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 1102s 44ms/step - loss: 0.3683 - acc: 0.8382 - val_loss: 0.4023 - val_acc: 0.8192\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 1018s 41ms/step - loss: 0.3397 - acc: 0.8546 - val_loss: 0.4308 - val_acc: 0.8080\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 1050s 42ms/step - loss: 0.3171 - acc: 0.8674 - val_loss: 0.3966 - val_acc: 0.8268\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 1028s 41ms/step - loss: 0.3159 - acc: 0.8677 - val_loss: 0.4075 - val_acc: 0.8270\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 1041s 42ms/step - loss: 0.2800 - acc: 0.8838 - val_loss: 0.4191 - val_acc: 0.8230\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 1038s 42ms/step - loss: 0.2551 - acc: 0.8961 - val_loss: 0.5255 - val_acc: 0.7939\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 1121s 45ms/step - loss: 0.2527 - acc: 0.8968 - val_loss: 0.4181 - val_acc: 0.8259\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 1015s 41ms/step - loss: 0.2290 - acc: 0.9066 - val_loss: 0.4241 - val_acc: 0.8309\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 1041s 42ms/step - loss: 0.2080 - acc: 0.9137 - val_loss: 0.4300 - val_acc: 0.8286\n",
      "f1 (train):  0.96624\n",
      "f1 (test):  0.82864\n"
     ]
    }
   ],
   "source": [
    "model_2=train_and_eval(cnn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
