{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eunbeejang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eunbeejang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For Data Preparation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # regular expressions\n",
    "\n",
    "\n",
    "# To clean up texts\n",
    "import nltk.data\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "# For Word Embedding\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import gensim.models as g\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# For the Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Bidirectional,Dropout, Input, SpatialDropout1D, CuDNNLSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "\n",
    "import logging\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "import tensorflowjs as tfjs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "IMDB_train = pd.read_csv('./IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train = IMDB_train[:][0]\n",
    "#IMDB_test = IMDB_test[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I saw the movie with two grown children. Altho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You're using the IMDb.&lt;br /&gt;&lt;br /&gt;You've given...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This was a good film with a powerful message o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Made after QUARTET was, TRIO continued the qua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For a mature man, to admit that he shed a tear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  For a movie that gets no respect there sure ar...  1\n",
       "1  Bizarre horror movie filled with famous faces ...  1\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...  1\n",
       "3  It's a strange feeling to sit alone in a theat...  1\n",
       "4  You probably all already know this by now, but...  1\n",
       "5  I saw the movie with two grown children. Altho...  1\n",
       "6  You're using the IMDb.<br /><br />You've given...  1\n",
       "7  This was a good film with a powerful message o...  1\n",
       "8  Made after QUARTET was, TRIO continued the qua...  1\n",
       "9  For a mature man, to admit that he shed a tear...  1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMDB_train = preprocessing(IMDB_train)\n",
    "IMDB_test = preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a sentence into a list of words\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # Convert words to lower case and split them\n",
    "    words = sentence.lower().split()\n",
    "    # Lemmatizing\n",
    "    #words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # 6. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whole data into a list of sentences where each sentence is a list of word items\n",
    "def list_of_sentences(data):\n",
    "    sentences = []\n",
    "    for i in data:\n",
    "        sentences.append(sentence_to_wordlist(i))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = list_of_sentences(IMDB_train)\n",
    "train_y = IMDB_train_y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 35674\n"
     ]
    }
   ],
   "source": [
    "# Create Word Vectors\n",
    "\n",
    "wv_model = Word2Vec(size=128, window=5, min_count=4, workers=4)\n",
    "\n",
    "wv_model.build_vocab(train_x) \n",
    "wv_model.train(train_x, total_examples=wv_model.corpus_count, epochs=wv_model.iter)\n",
    "word_vectors = wv_model.wv\n",
    "words = list(wv_model.wv.vocab)\n",
    "\n",
    "# Calling init_sims will make the model will be better for memory\n",
    "# if we don't want to train the model over and over again\n",
    "wv_model.init_sims(replace=True)\n",
    "\n",
    "#n_words = print(len(words))\n",
    "\n",
    "print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))\n",
    "\n",
    "# save model\n",
    "wv_model.wv.save_word2vec_format('model.txt', binary=False)\n",
    "\n",
    "# load model\n",
    "#new_model = Word2Vec.load('model.bin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Need the interactive Tools for Matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "new_model = KeyedVectors.load_word2vec_format('model.txt')\n",
    "#model.save_word2vec_format('model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35675, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#glove2word2vec(glove_input_file=\"model.txt\", word2vec_output_file=\"model.word2vec.txt\")\n",
    "#glove2word2vec(glove_input_file=\"out_vec.txt\", word2vec_output_file=\"retrofittedglove.word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = KeyedVectors.load_word2vec_format('model.txt')\n",
    "retrofitted_model = KeyedVectors.load_word2vec_format('out_vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_words = list(retrofitted_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build dictionary & inv_vocab\n",
    "\n",
    "def create_vocab(data_collect, max_vocab):\n",
    "    # Get raw data\n",
    "    x_list = data_collect\n",
    "    sample_count = sum([len(x) for x in x_list])\n",
    "    words = []\n",
    "    for data in x_list:\n",
    "        words.extend([data])\n",
    "    count = Counter(words) # word count\n",
    "    inv_vocab = [x[0] for x in count.most_common(max_vocab)]\n",
    "    vocab = {x: i for i, x in enumerate(inv_vocab, 1)}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = create_vocab(words, len(words))\n",
    "ret_vocab, ret_inv_vocab = create_vocab(new_words, len(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35674"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the max length sentence\n",
    "def find_max_length_sentence(sentence):\n",
    "    max_length = 0\n",
    "    for i in sentence:\n",
    "        length = len(sentence_to_wordlist(i))\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450\n"
     ]
    }
   ],
   "source": [
    "seq_length = find_max_length_sentence(IMDB_train)\n",
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map each word to corresponding vector\n",
    "def map_to_vec(word):\n",
    "    vec = wv_model[word]\n",
    "    return vec\n",
    "\n",
    "#map_to_vec('care')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "def make_emb_matrix(inv_vocab):\n",
    "    emb_matrix = []\n",
    "    for word in inv_vocab:\n",
    "        emb_matrix.append(map_to_vec(word))\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embedding = np.asarray(make_emb_matrix(inv_vocab))\n",
    "ret_embedding = np.asarray(make_emb_matrix(ret_inv_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Word Embeddings in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 100\n",
    "num_words = len(word_vectors.vocab)\n",
    "vocab = Counter(words)\n",
    "ret_vocab = Counter(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(num_words-1))}\n",
    "\n",
    "train_sequences = [[word_index.get(t, 0) for t in sentence]\n",
    "             for sentence in IMDB_train[:len(IMDB_train)]]\n",
    "\n",
    "test_sequences = [[word_index.get(t, 0)\n",
    "                   for t in sentence] for sentence in IMDB_test[:len(IMDB_test)]]\n",
    "\n",
    "# Pad zeros to match the size of matrix\n",
    "train_data = pad_sequences(train_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")\n",
    "test_data = pad_sequences(test_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(num_words, wv_dim) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ret_word_index = {t[0]: i+1 for i,t in enumerate(ret_vocab.most_common(num_words-1))}\n",
    "\n",
    "ret_train_sequences = [[ret_word_index.get(t, 0) for t in sentence]\n",
    "             for sentence in IMDB_train[:len(IMDB_train)]]\n",
    "\n",
    "ret_test_sequences = [[ret_word_index.get(t, 0)\n",
    "                   for t in sentence] for sentence in IMDB_test[:len(IMDB_test)]]\n",
    "\n",
    "# Pad zeros to match the size of matrix\n",
    "ret_train_data = pad_sequences(ret_train_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")\n",
    "ret_test_data = pad_sequences(ret_test_sequences, maxlen=seq_length, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the matrix with random numbers\n",
    "ret_wv_matrix = (np.random.rand(num_words, wv_dim) - 0.5) / 5.0\n",
    "for word, i in ret_word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        ret_embedding_vector = ret_word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        ret_wv_matrix[i] = ret_embedding_vector\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "wv_layer = Embedding(num_words,\n",
    "                     wv_dim,\n",
    "                     mask_zero=False,\n",
    "                     weights=[wv_matrix],\n",
    "                     input_length=seq_length,\n",
    "                     trainable=False)\n",
    "\n",
    "ret_wv_layer = Embedding(num_words,\n",
    "                     wv_dim,\n",
    "                     mask_zero=False,\n",
    "                     weights=[ret_wv_matrix],\n",
    "                     input_length=seq_length,\n",
    "                     trainable=False)\n",
    "\n",
    "# Inputs\n",
    "comment_input = Input(shape=(seq_length,), dtype='int64')\n",
    "embedded_sequences = wv_layer(comment_input) # regular word2vec\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "\n",
    "\n",
    "#ret_embedded_sequences = ret_wv_layer(comment_input)# retrofitted word2vec\n",
    "\n",
    "\n",
    "# LSTM\n",
    "x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n",
    "\n",
    "# Output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[comment_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',   #binary_crossentropy\n",
    "              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "hist = model.fit(train_data, IMDB_train_y, validation_data=(test_data, IMDB_test_y), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
